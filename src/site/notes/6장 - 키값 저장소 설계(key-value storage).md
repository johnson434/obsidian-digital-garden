---
{"dg-publish":true,"permalink":"/6-key-value-storage/","noteIcon":""}
---

# Tags
- [[03 - Tags/가상 면접 사례로 배우는 대규모 시스템 설계 기초 1권\|가상 면접 사례로 배우는 대규모 시스템 설계 기초 1권]]
---
# 단서 질문

---
# 핵심 요약

---
# 핵심 필기
## 키값 저장소란?
데이터를 key,value 형태로 저장하는 시스템이다. Redis나 AWS의 DynamoDB[^1] 등이 있다.
## 요구 사항
- key와 value 데이터의 총합은 10KB
- 큰 데이터 저장 가능
- 높은 가용성을 제공. 시스템에 장애가 발생해도 빨리 응답해야 한다.
- 높은 규모 확장성을 제공해야 한다. 트래픽 양에 따라 서버 증설/삭제가 가능
- 데이터 일관성 수준을 조정할 수 있다.
- 응답 지연 시간이 짧아야 한다.
## 해결 방법
### 1. 단일 서버
당연하게도 가장 간단한 방법이다. 단일 서버에 메모리에 해시테이블로 모든 데이터를 저장한다.
만약, 메모리가 부족하다면 데이터를 압축하거나 접근 빈도가 낮은 데이터들을 디스크에 저장한다.
간단한 방법이지만 서버 한 대가 죽으면 모든 서비스가 다운되므로 가용성이 낮으며, 이후 트래픽이 증가함에 따라서 확장성이 커지기 힘들다. 모두 단일 서버에 의존하기 때문이다.
### 분산 키-값 저장소
분산 키-값 저장소는 분산 해시 테이블이라고 불린다. 여러 키-값을 서로 다른 서버에 분산 시키는 방법이다. 분산 시스템을 설계하기 위해선 CAP 정리를 이해할 필요가 있다.
#### CAP 정리
일관성(consistency), 가용성(availability), 파티션 감내(Partition Tolerance)를 의미한다.
일관성: 분산 시스템에 접속하여도 동일한 데이터를 보장한다.
가용성: 일부 노드에 장애가 발생하여도 서비스가 이용 가능하다.
파티션 감내: 분산 시스템의 노드 간에 통신이 불가능한 상태가 발생하여도 시스템은 계속 동작한다.[^2]

CAP 정리는 여기서 어떤 두 가지를 충족하려면 한 가지 희생이 필요하다는 것을 의미한다.[^3]

### 시스템 컴포넌트
키-값 저장소 구현에 사용될 핵심 컴포넌트와 기술들
- 데이터 파티션
- 데이터 다중화(replication)
- 일관성(consistency)
- 일관성 불일치 해소(inconsistency resolution)
- 장애 처리
- 시스템 아키텍처 다이어그램
- 쓰기 경로(write path)
- 읽기 경로(read path)
이번 절에서 구현하는 내용은 다이나모, 카산드라, 빅테이블의 사례를 참고한 것
#### 데이터 파티션
대규모 어플레킹션에서 전체 데이터를 한 대 서버에 욱여넣긴 사실상 불가능하다.
따라서, 데이터를 파티션을 통해 분할할 필요가 있다.
**데이터 파티션 시에 고려 사항**은 두 가지가 있다. 
첫 번째로 데이터를 고르게 분할할 수 있는가? 

두 번째로 노드가 추가되거나 삭제됐을 때, 데이터가 최소한으로 이동해야 한다.
이전에 배운 안정 해시 알고리즘이 해당 조건을 만족하는데 아주 적합한 방법이다.

데이터 파티션의 이점은 다음과 같다.
- 노드의 추가 및 삭제가 가능하다.
- 노드가 다양한 스펙을 가질 수 있다. 더 높은 사양을 가진 노드엔 가상 노드의 개수를 늘려서 더 많은 key를 저장하도록 만들 수 있다.
#### 데이터 다중화
높은 가용성과 안정성을 보장하기 위해서 데이터를 N개 서버에 비동기적으로 다중화한다. N개 서버를 선택하는 방법은 안정해시 알고리즘의 링을 사용한다.
![Pasted image 20250305233947.png](/img/user/image/Pasted%20image%2020250305233947.png)
기존엔 James가 db1에 저장되었다. 데이터 다중화를 위해 2개의 DB에 저장할 땐, James가 db1가 db2에 저장된다.
위 그림을 보면 알겠지만 N이 전체 물리 서버의 개수보다 커지면 안된다. 그렇게 되면 동일한 서버의 동일한 데이터가 중복되어 저장될 수 있다.
#### 데이터 일관성
비동기 데이터 복제로 인한 데이터의 일관성은 어떻게 보장할까? 일관성을 보장하기 위해서 다중화 된 데이터는 적절한 동기화가 필요하다.
정족수 합의(Quorum Consensus) 프로토콜을 사용하면 읽기/쓰기 연산 모두에 일관성을 보장할 수 있다. 정족수 합의와 관련된 정의부터 살펴보겠습니다.
![Pasted image 20250307094627.png](/img/user/image/Pasted%20image%2020250307094627.png)
**중재자(coordinator)**는 클라이언트와 DB 서버들 사이에서 중재자 역할을 합니다.
정족수 합의 규칙에 맞게 데이터 작업을 한 후에 클라이언트에게 응답을 반환합니다.

- 정족수 합의 규칙에 필요한 Param들
	- N = 사본 개수
	- W = 쓰기 연산에 대한 정족수. 쓰기 연산이 성공한 것으로 간주되려면 적어도 W개의 서버로부터 쓰기 연산이 성공했다는 응답을 받아야 한다.[^4]
	- R = 읽기 연산에 대한 정족수. 읽기 연산이 성공한 것으로 간주되려면 적어도 R개의 서버로부터 응답을 받아야 한다.
![Pasted image 20250307092655.png](/img/user/image/Pasted%20image%2020250307092655.png)
위 예제는 W=1인 경우다. 
1. 클라이언트가 중재자에 write 요청을 보낸다.
2. 중재자가 s0 ~ s2에 데이터를 저장한다.
3. Ack 응답이 1개 올 때까지 기다린다.
4. DB 서버로부터 응답을 받으면 중재자는 다음 응답을 기다리지 않고 클라이언트에게 응답을 반환한다.

R도 위와 마찬가지다. 
1. 클라이언트가 중재자에 read 요청을 보낸다.
2. 중재자가 s0 ~ s2에서 데이터를 조회한다.
3. 읽기 응답이 1개 올 때까지 기다린다.
4. DB 서버로부터 응답을 받으면 중재자는 다음 응답을 기다리지 않고 클라이언트에게 응답을 반환한다.

당연히 W의 값이 높으면 최대한 많은 노드에 데이터 일관성이 보장된다. 그러나, 읽기 작업이 모든 서버에 읽기를 끝마칠 때까지 기다리므로 읽기 작업에 많은 시간이 소요될 것이다.
W의 값이 작다면 최소한의 노드에 데이터 일관성이 보장된다. 따라서, 각 DB 서버의 데이터 일관성이 깨질 가능성이 높다. R도 마찬가지다. 

이를 대략적으로 나타내면 아래와 같다.[^5]
- R = 1, W = N: 빠른 읽기 연산에 최적화된 시스템
- R = N, W = 1: 빠른 쓰기 연산에 최적화된 시스템
- W + R > N: 강한 일관성이 보장됨 (보통 N = 3, W=R=2)[^6]
- W + R <= N: 강한 일관성이 보장되지 않는다.
#### 일관성 모델
일관성 모델(consistency model)은 키-값 저장소를 설계할 때, 고려해야 할 또 하나의 중요한 요소다.
- 강한 일관성(strong consistency): 모든 읽기 연산은 가장 최근에 갱신된 결과를 반환한다.
- 약한 일관성(weak consistency): 읽기 연산은 가장 최근에 갱신된 결과를 반환하지 못한다.
- 결과적 일관성(eventual consistency): 약한 일관성의 한 형태로, 갱신 결과가 결국에는(eventually) 모든 사본에 반영(동기화)되는 모델이다.[^7]

강한 일관성을 달성하는 일반적인 방법은, 모든 사본에 현재 쓰기 연산의 결과가 반영될 때까지 해당 데이터에 대한 읽기/쓰기를 금지하는 방법이다.
우리는 Dynamo나 Cassandra에서 사용하는 결과적 일관성을 설계할 것이다. 이 방법은 데이터의 일관성이 깨지기 쉬운데 이 문제는 클라이언트가 데이터 버전 정보를 사용하여 해결한다.

##### 비일관성 해소 기법: 데이터 버저닝
버저닝(versioning)과 벡터 시계(vector clock)를 활용한다.
벡터 시계는 데이터를 `저장된 서버`와 `버전`으로 기록하는 방법이다. 어떤 버전이 선행 버전인지, 후행 버전인지, 다른 버전과 충돌이 있는지 판별한다.
![Pasted image 20250307111539.png](/img/user/image/Pasted%20image%2020250307111539.png)
Si: 데이터가 저장된 서버
vi: 데이터의 버전

벡터 시계열의 명확한 단점은 두 가지가 있다.
1. 클라이언트에서 충돌 해소 로직을 작성 -> 복잡하다.
2. 버전이 너무 길어지면 오버헤드 발생.[^8]

#### 장애 처리
##### 장애 감지
분산 시스템에서 서버가 장애 처리가 되려면 보통 두 대 이상의 서버에서 해당 서버의 장애를 보고해야 한다.
![Pasted image 20250307112609.png](/img/user/image/Pasted%20image%2020250307112609.png)
 서버의 장애를 탐지하는 가장 간단한 방법은 모든 노드가 멀티캐스팅(multicasting) 채널을 구축하는 설계다.[^9]
 이러한 형식은 비효율적이다. 따라서, 가십 프로토콜(gossip protocol) 같은 분산형 장애 감지(decentralized failure) 솔루션을 채택한다.

**가십 프로토콜**
- 각 노드는 멤버쉽 목록을 유지한다. 멤버쉽엔 멤버 ID와 박동 카운터(heartbeat count)로 이뤄진다.
- 각 노드는 주기적으로 자신의 박동 카운터를 증가시킨다.
- 각 노드는 무작위로 선정된 노드들에게 주기적으로 자기 박동 카운터 목록을 보낸다.
- 박동 카운터를 수신한 노드는 해당 값으로 멤버쉽 목록을 갱신한다.
- 어떤 멤버의 박동 카운터 값이 지정된 시간 동안 갱신되지 않으면 해당 서버를 장애로 판단한다.
![Pasted image 20250307113501.png](/img/user/image/Pasted%20image%2020250307113501.png)
##### 일시적 장애 처리
엄격한 정족수(strict quorum) 접근법을 사용한다면 모든 읽기/쓰기를 중단한다.
느슨한 정족수(sloppy quorum) 접근번에선 쓰기 연산을 수행할 W개의 서버와 R개의 서버를 해시 링에서 고른다.
네트워크나 서버 문제로 장애 상태인 서버로 가는 요청은 다른 서버가 잠시 맡아 처리한다. 이후에 장애 서버가 복구되면 일괄 반영한다. 이 때, 일시적으로 임시적으로 처리한 서버에 단서(hint)를 남겨둔다. 이러한 방법을 임시 위탁(hinted handoff) 기법이라고 말한다.
![Pasted image 20250307113954.png](/img/user/image/Pasted%20image%2020250307113954.png)
위 예제는 s2에 장애가 발생했을 때, s3에서 작업을 처리한다. 이후에 s2가 복구되면 s3에서 진행된 작업을 s2에서 진행한다.
##### 영구 장애 처리
반-엔트로피(anti-entropy) 프로토콜을 구현한다.

---
# 참고 자료
 - http://eincs.com/2013/07/misleading-and-truth-of-cap-theorem/
# 주석

[^1]: Serverless 문서형 DB다. DocumentDB와는 달리 서버가 존재하지 않는다.

[^2]: 이걸 왜 Partition이라고 명칭할까? 내 생각엔 네트워크가 분리되어 통신이 불가능한 것을 파티션되었다고 생각하여 지은 명칭 같다. 서로 통신이 가능한 네트워크에서 통신이 불가능한 서로 다른 네트워크로 분리된 느낌이니까 파티션이라고 표현하는 듯하다.

[^3]: 단, CAP 정리는 실제 설계와 괴리감이 있다는 얘기가 많다. 따라서, PACELC가 더 부합하다고 본다.[[Excalidraw/보류/분산 시스템과 설계 원칙(CAP, PACELC)\|분산 시스템과 설계 원칙(CAP, PACELC)]]

[^4]: aurora의 경우엔 3개의 AZ에 2개씩 총 6개의 데이터 백업이 만들어지는데, 이 중의 4개 이상이 살아있어야 쓰기 작업이 가능하며 3개 이상이 살아있어야 읽기가 가능하다.

[^5]: 책에선 이런 식으로 W와 R이 세트처럼 묶여있지만, 난 독립적인 조건이므로 묶어서 설명할 필요가 없다고 생각한다.
	W가 높으면 쓰기 일관성이 높아지는 것이고 R이 높으면 읽기 작업을 했을 때, 클러스터에서 가장 일관적인 데이터를 반환한다.
	W가 낮으면 쓰기 일관성이 낮아지지만 속도는 빨라지고, R이 낮으면 쓰기 작업이 진행되지 않았거나, 일관성이 어긋난 DB의 데이터를 조회할 가능성이 높아진다.
	이 정도로 이해하면 될 것 같다. 서로 다른 값이지만 원론적으로 몇 개의 서버에서 응답을 기다릴 지가 핵심이고 별개로 설정해줘야 할 값이다. 

[^6]: 강한 일관성을 보장한다는 데 W=1이고 R=3이라면? 과반수 합의를 통해서 최신 데이터가 아닌 값이 응답될 수 있다. 애매하다.

[^7]: 읽기 복제본들이 결과적 일관성을 사용한다. 빠른 읽기 속도를 보장하기 위해서 비동기 복제한다. 클라이언트가 요청을 보냈을 때, 복제본 DB 서버에 따라서 데이터가 일치하지 않을 수 있지만 결과적으로(eventually) 데이터는 동일해진다.

[^8]: 단, AWS의 DynamoDB에선 실제 서비스에서 문제가 발생한 적이 없다고 한다.

[^9]: Mesh형 네트워크 구성
